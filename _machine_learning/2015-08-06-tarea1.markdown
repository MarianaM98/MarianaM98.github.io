---
layout: post
title: Introduccion Al Modelo De Aprendizaje PAC
date: 2019-08-13 07:59:00
mathjax: true
---
## Tarea 1 machine Learning 

1. Especifica una parametrización del espacio de hipótesis.

$h(t,m,n)\in H$ donde
$t$: la esquina inferior izquierda de $h$ se encuentra en $(t,t)$
$m$: alto de $h$ (multiplo de 0.01)
$n$: ancho de $h$ (multiplo de 0.01)

2. ¿Cuál es el tamaño del espacio de hipótesis?

$t=200$

$m=\frac{2-t}{0.01}$

$n=\frac{2-t}{0.01}$

El tamaño del espacio de hipótesis es $mn=\frac{(2-t)^2}{0.0001}$ para $t=0.01,0.01,...,2$ $\Rightarrow \sum_{i=1}^{200}{(200-i)^2}$ 

3. Calcula la probabilidad de extraer un ejemplo en cada una de las clases.

La probabilidad de extraer $h_0(t_0,m_0,n_0)$ es $\frac{m_0n_0}{4}$

4. En python, define un conjunto de métodos que realicen las siguientes tareas: 

a. Formen conjuntos de entrenamiento $S$ con $m$ elementos
{% highlight python %}

def entrenamiento(k): 
    x=np.random.uniform(0,2,k)
    y=np.random.uniform(0,2,k)
    z=np.zeros(k)
    p=[]
    for i in range(0,k):
        if x[i]<(1+1/sqrt(2)) and x[i]>(1-1/sqrt(2)):
            if y[i]<(1+1/sqrt(2)) and y[i]>(1-1/sqrt(2)):
                z[i]=1
            else:
                z[i]=0
        else: 
            z[i]=0
    for i in range(0,k):
        p=p+[x[i],y[i],z[i]]
    return p

{% endhighlight %}

b. Implementen una hipótesis $h:X\to \{0,1\}$ para cada $h\in X$
{% highlight python %}

def hipotesis(t,m,n,p):
    if p[0]<(n+t) and p[0]>t:
        if p[1]<(m+t) and p[1]>(t):
                p[2]=1
        else:
                p[2]=0
    else: 
        p[2]=0
    return p[2]
    
{% endhighlight %}

c. Calculen error empíricos y de generalización
{% highlight python %}

def errorEmpirico(k,t,m,n):
    S=entrenamiento(k)
    x1=np.zeros(k)
    y1=np.zeros(k)
    z1=np.zeros(k)
    for c in range(0,k):
        x1[c]=S[3*c]
        y1[c]=S[3*c+1]
        z1[c]=S[3*c+2]
    contador=0
    p=[0,0,0]
    for i in range(0,k):
        p[0]=x1[i]
        p[1]=y1[i]
        w=hipotesis(t,m,n,p)
        if z1[i]!=w:
            contador=contador+1 
    errorE=contador/k
    return errorE

def errorGeneralizacion(t,m,n):
    area=m*n
    n0=0
    m0=0
    if t<1-1/sqrt(2):
        if n+t>1-1/sqrt(2) and n+t<1+1/sqrt(2):
            n0=n+t-(1-1/sqrt(2))
        elif n+t>1-1/sqrt(2) and n+t>1+1/sqrt(2):
            n0=2/sqrt(2)
    elif t>1-1/sqrt(2):
        if n+t<1+1/sqrt(2):
            n0=n
        elif n+t>1+1/sqrt(2):
            n0=1+1/sqrt(2)-t
    if t<1-1/sqrt(2):
        if m+t>1-1/sqrt(2) and m+t<1+1/sqrt(2):
            m0=n+t-(1-1/sqrt(2))
        elif m+t>1-1/sqrt(2) and m+t>1+1/sqrt(2):
            m0=2/sqrt(2)
    elif t>1-1/sqrt(2):
        if m+t<1+1/sqrt(2):
            m0=m
        elif m+t>1+1/sqrt(2):
            m0=1+1/sqrt(2)-t
    if t>1-1/sqrt(2) and t<1+1/sqrt(2):
        m0=2/sqrt(2)
        n0=2/sqrt(2)
    area2=m0*n0
    errorG=abs((area-area2))/4
    return errorG

{% endhighlight %}

c. Seleccione el rectángulo $ERM_H$
{% highlight python %}

def ERMh(errorEmpir):
    errorEmpir=errorEmpir.tolist()
    ermh=errorEmpir.index(min(errorEmpir))
    return(ermh)
    
{% endhighlight %}

5. Simplifique la clase de hipótesis finita a rectángulos con lados paralelos a los lados de $X$, centrados en $X$. ¿Cuál es el tamaño de $H$? Forma un conjunto de entrenamiento con $m$ elementos, para $m=e^0,e^{1/2},e^1,...,m_H$ donde $m_H$ es la complejidad de $S$ para un parámetro de confianza y un error de 0.01. Grafica los puntos de cada conjunto de entrenamiento. Grafica los errores de generalización $L_{(D,f)}$ y empírico $L_S$ para cada hipótesis en $H$.  Obtén algún rectángulo $h_S$ que minimice $L_S$  y grafícalo en $X$.

El tamaño de $H$ es $\frac{1}{0.01}=100$
Conjuntos de entrenamiento:

<div class="img_row">
	<img class="col one" src="/img/entrenamiento1.jpg">
	<img class="col one" src="/img/entrenamiento2.jpg">
	<img class="col one" src="/img/entrenamiento3.jpg">
</div>

<div class="img_row">
	<img class="col one" src="/img/entrenamiento4.jpg">
	<img class="col one" src="/img/entrenamiento5.jpg">
	<img class="col one" src="/img/entrenamiento6.jpg">
</div>

<div class="img_row">
	<img class="col one" src="/img/entrenamiento7.jpg">
	<img class="col one" src="/img/entrenamiento8.jpg">
	<img class="col one" src="/img/entrenamiento9.jpg">
</div>

<div class="img_row">
	<img class="col one" src="/img/entrenamiento10.jpg">
	<img class="col one" src="/img/entrenamiento11.jpg">
	<img class="col one" src="/img/entrenamiento12.jpg">
</div>

<div class="img_row">
	<img class="col one" src="/img/entrenamiento13.jpg">
	<img class="col one" src="/img/entrenamiento14.jpg">
	<img class="col one" src="/img/entrenamiento15.jpg">
</div>

Graficas de errores:

<div class="img_row">
	<img class="col one" src="/img/error1.jpg">
	<img class="col one" src="/img/error2.jpg">
</div>

Rectángulo $h_S$ que minimice $L_S$
<div class="img_row">
	<img class="col one" src="/img/min.jpg">
</div>

6. Para cada valor de $m$, forma 1000 conjuntos de entrenamiento, selecciona $h_S$ en cada caso y haz un histograma del error $L_{(D,f)}$

<div class="img_row">
	<img class="col one" src="/img/his1.png">
	<img class="col one" src="/img/his2.png">
	<img class="col one" src="/img/his3.png">
</div>

<div class="img_row">
	<img class="col one" src="/img/his4.png">
	<img class="col one" src="/img/his5.png">
	<img class="col one" src="/img/his6.png">
</div>

<div class="img_row">
	<img class="col one" src="/img/his7.png">
	<img class="col one" src="/img/his8.png">
	<img class="col one" src="/img/his9.png">
</div>

<div class="img_row">
	<img class="col one" src="/img/his10.png">
	<img class="col one" src="/img/his11.png">
	<img class="col one" src="/img/his12.png">
</div>

<div class="img_row">
	<img class="col one" src="/img/his13.png">
	<img class="col one" src="/img/his14.png">

</div>
