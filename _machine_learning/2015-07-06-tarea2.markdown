---
layout: post
title: PAC Generalizado Y Predictores Lineales
date: 2019-08-20 07:59:00
mathjax: true
---
1. Implemente el algoritmo

{% highlight python %}
def perceptron(S,y,m,d):
    w=[0]*(d+1)
    for i in range(0,m-1):
        t=0
        for j in range(0,d-1):
            t=t+w[j+1]*S[i][j]
        t=t+w[0]
        if t!=y[i]:
            for j in range(0,d-1):
                S[i][j]=S[i][j]*y[i]
                w[j+1]=w[j+1]+S[i][j]
            w[0]=w[0]*y[i]
        else:
            return(w)
{% endhighlight %}

2.Consideremos ahora la clasificación binaria en $\mathbb{R}^2$, bajo el supuesto de realizabilidad en la clase de predictores lineales. Supongamos que el predictor verdadero está dado por $w=(-1,1,1)$. Grafique la frontera entre ambas clases de acuerdo al clasificador. 

<div class="img_row">
	<img class="col one" src="/img/vector2.png">
</div>

3. Supón que la distribución sobre $\mathbb{R}^2$ es homogénea. ¿Cómo generas puntos aleatorios en $\mathbb{R}^2$ ? Para cada $m=1,..,1000$ , forma 100 conjuntos de entrenamiento $S_m$ y grafica el promedio de $R=max_i$ ($|x_i|$). Es decir ¿cómo se escala $R$ con el tamaño de muestra?

Los puntos se generaron en los rangos [-100,100]x[-100,100]

<div class="img_row">
	<img class="col one" src="/img/R.jpg">
</div>

4. Para cada $m=1,...,1000$ ejecuta el algoritmo de percpetrón y grafica el número de pasos necesarios que le toma al algoritmo encontrar un vector $w*$ como función de $m$.

5. Para m=100 grafica en $\mathbb{R}^2$ la secuencia de actualizaciones $w^(t)$ de predictores hasta que clasifican correctamente a los datos de entrenamiento. 
