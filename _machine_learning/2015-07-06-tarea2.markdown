---
layout: post
title: PAC Generalizado Y Predictores Lineales
date: 2019-08-20 07:59:00
mathjax: true
---
1
Implemente el algoritmo

{% highlight python %}
def perceptron(S,y,m,d):
    w=[0]*(d+1)
    for i in range(0,m-1):
        t=0
        for j in range(0,d-1):
            t=t+w[j+1]*S[i][j]
        t=t+w[0]
        if t!=y[i]:
            for j in range(0,d-1):
                S[i][j]=S[i][j]*y[i]
                w[j+1]=w[j+1]+S[i][j]
            w[0]=w[0]*y[i]
        else:
            return(w)
{% endhighlight %}

# 2
Consideremos ahora la clasificación binaria en $\mathbb{R}^2$, bajo el supuesto de realizabilidad en la clase de predictores lineales. Supongamos que el predictor verdadero está dado por $w=(-1,1,1)$. Grafique la frontera entre ambas clases de acuerdo al clasificador. 

<div class="img_row">
	<img class="col one" src="/img/vector2.png">
</div>

# 3
Supón que la distribución sobre $\mathbb{R}^2$ es homogénea. ¿Cómo generas puntos aleatorios en $\mathbb{R}^2$ ? Para cada $m=1,..,1000$ , forma 100 conjuntos de entrenamiento $S_m$ y grafica el promedio de $R=max_i$ ($|x_i|$). Es decir ¿cómo se escala $R$ con el tamaño de muestra?

Los puntos se generaron en los rangos [-100,100]x[-100,100]

{% highlight python %}
r=[0]*100    
def muestra(m):
    S=np.zeros([m,2])
    y=[0]*m
    r=[0]*m
    for i in range(0,m):
        for j in range(0,2):
            S[i][j]=random.uniform(-100,100)
    for i in range(0,m):
        t=-1*1+1*S[i][0]+1*S[i][1]
        if t<0:
            y[i]=-1
        else:
            y[i]=1
    for i in range(0,m):
        r[i]=sqrt(1+S[i][0]*S[i][0]+S[i][1]*S[i][1])
    rm=np.max(r)
    return(S,y,rm)

m=[0]*100
for i in range(1,101):
    m[i-1]=i

for i in range(1,101):
    h=0
    for j in range(0,100):
        h=muestra(i)[2]+h
    h=h/100
    r[i-1]=h
plt.xlabel("Tamaño de muestra")
plt.ylabel("R")   
plt.plot(r,m)
{% endhighlight %}

<div class="img_row">
	<img class="col one" src="/img/R.jpg">
</div>

# 4
Para cada $m=1,...,1000$ ejecuta el algoritmo de percpetrón y grafica el número de pasos necesarios que le toma al algoritmo encontrar un vector $w*$ como función de $m$.

{% highlight python %}
def iter(m):
    it=[0]*m
    n=[0]*m
    for i in range(1,m+1):
        n[i-1]=i
    for i in range(1,m+1):
        H=muestra(i)
        S=H[0]
        y=H[1]
        it[i]=perceptron(S,y,i,2)[1]
    plt.bar(n,it)
    return(n,it)
{% endhighlight %}

# 5
Para m=100 grafica en $\mathbb{R}^2$ la secuencia de actualizaciones $w^(t)$ de predictores hasta que clasifican correctamente a los datos de entrenamiento. 
<div class="img_row">
	<img class="col one" src="/img/100.png">
</div>
